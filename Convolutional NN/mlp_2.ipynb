{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: initial, loss: 2.3636390898/2.36419412254 accuracy: 0.0888166666667/0.0888\n",
      "....................\n",
      "epoch: 1, loss: 1.92725270421/1.92829992427 accuracy: 0.715033333333/0.7075\n",
      "....................\n",
      "epoch: 2, loss: 1.7427773999/1.74052244741 accuracy: 0.801483333333/0.8025\n",
      "....................\n",
      "epoch: 3, loss: 1.64441840235/1.64323781534 accuracy: 0.88295/0.8816\n",
      "....................\n",
      "epoch: 4, loss: 1.61025984146/1.61057674996 accuracy: 0.903733333333/0.8994\n",
      "....................\n",
      "epoch: 5, loss: 1.59061815899/1.59223122078 accuracy: 0.916333333333/0.9101\n",
      "....................\n",
      "epoch: 6, loss: 1.57759444952/1.58035841192 accuracy: 0.924483333333/0.9162\n",
      "....................\n",
      "epoch: 7, loss: 1.56775405465/1.57164836579 accuracy: 0.93115/0.9203\n",
      "....................\n",
      "epoch: 8, loss: 1.56003996561/1.56495551731 accuracy: 0.936166666667/0.9246\n",
      "....................\n",
      "epoch: 9, loss: 1.55370964164/1.55958307858 accuracy: 0.9403/0.9285\n",
      "....................\n",
      "epoch: 10, loss: 1.5483820976/1.55493738044 accuracy: 0.943766666667/0.9319\n",
      "....................\n",
      "epoch: 11, loss: 1.54377702856/1.55100199837 accuracy: 0.946683333333/0.9336\n",
      "....................\n",
      "epoch: 12, loss: 1.53984860418/1.54774932788 accuracy: 0.949666666667/0.9366\n",
      "....................\n",
      "epoch: 13, loss: 1.53635946022/1.54485974684 accuracy: 0.951966666667/0.9394\n",
      "....................\n",
      "epoch: 14, loss: 1.53330031259/1.54240580526 accuracy: 0.95445/0.9413\n",
      "....................\n",
      "epoch: 15, loss: 1.53053954192/1.54026862111 accuracy: 0.9564/0.943\n",
      "....................\n",
      "epoch: 16, loss: 1.52797178163/1.53826626163 accuracy: 0.958033333333/0.944\n",
      "....................\n",
      "epoch: 17, loss: 1.5256962206/1.53653963727 accuracy: 0.959666666667/0.945\n",
      "....................\n",
      "epoch: 18, loss: 1.52355684843/1.53491117533 accuracy: 0.96085/0.9465\n",
      "....................\n",
      "epoch: 19, loss: 1.52157205807/1.53339133787 accuracy: 0.96205/0.9467\n",
      "....................\n",
      "epoch: 20, loss: 1.51970045603/1.531952586 accuracy: 0.963333333333/0.9469\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1159e20d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11656f390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "import gzip\n",
    "import os\n",
    "import cPickle as pickle\n",
    "import urllib\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# -------------- DATASETS ---------------\n",
    "# ---------------------------------------\n",
    "def load_XOR():\n",
    "    \"\"\"\n",
    "    Loads training data for XOR function. The outputs are encoded using one-hot encoding, so you can check softmax and\n",
    "    cross-entropy loss function.\n",
    "    :return: Pair of numpy arrays: (4, 2) training inputs and (4, 2) training labels\n",
    "    \"\"\"\n",
    "    X = np.asarray([\n",
    "        [0.0, 0.0],\n",
    "        [0.0, 1.0],\n",
    "        [1.0, 0.0],\n",
    "        [1.0, 1.0]], dtype=np.float32)\n",
    "    T = np.asarray([\n",
    "        [0.0, 1.0],\n",
    "        [1.0, 0.0],\n",
    "        [1.0, 0.0],\n",
    "        [0.0, 1.0]], dtype=np.float32)\n",
    "    return X, T\n",
    "\n",
    "\n",
    "def load_spirals():\n",
    "    '''\n",
    "    Loads training and testing data of the spiral dataset. The inputs are standardized and the output labels are one-hot encoded.\n",
    "    Source based on http://cs231n.github.io/\n",
    "    :return: Quadruple of numpy arrays (100, 2) training inputs, (100, 3) one-hot encoded training labels,\n",
    "        (100, 2) testing inputs and (100, 3) one-hot encoded testing labels\n",
    "    '''\n",
    "\n",
    "    def generate_points(N):\n",
    "        K = 3\n",
    "        X = np.zeros((N * K, 2), dtype=np.float32)\n",
    "        T = np.zeros((N * K, K), dtype=np.float32)\n",
    "        for i in xrange(K):\n",
    "            r = np.linspace(0.0, 2.5, N)\n",
    "            t = np.linspace(i * 4, (i + 1) * 4, N) + rng.randn(N) * 0.2\n",
    "            ix = range(N * i, N * (i + 1))\n",
    "            X[ix] = np.c_[r * np.sin(t), r * np.cos(t)]\n",
    "            T[ix, i] = 1.0  # one-hot encoding\n",
    "        return X, T\n",
    "\n",
    "    rng = np.random.RandomState(1234)\n",
    "    X_train, T_train = generate_points(100)\n",
    "    X_test, T_test = generate_points(100)\n",
    "    return X_train, T_train, X_test, T_test\n",
    "\n",
    "\n",
    "def plot_2D_classification(X, T, net):\n",
    "    \"\"\"\n",
    "    Plots a classification for 2D inputs. The call of this function should be followed by plt.show()\n",
    "    in non-interactive matplotlib session.\n",
    "    :param X: Input of shape (n_samples, 2)\n",
    "    :param T: One-hot encoded target labels of shape (n_samples, n_classes)\n",
    "    :param net: trained network, instance of MLP class\n",
    "    \"\"\"\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = net.propagate(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = np.argmax(Z, axis=1)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=np.argmax(T, axis=1), s=40, cmap=plt.cm.Spectral)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "\n",
    "\n",
    "def load_MNIST():\n",
    "    \"\"\"\n",
    "    Loads MNIST dataset. If not present locally, the dataset is downloaded from Yann LeCun's site.\n",
    "    The dataset consists of 60k training and 10k testing samples of 28x28 grayscale images. The inputs are standardized\n",
    "    and the output labels are one-hot encoded.\n",
    "    Inspired by https://gist.github.com/ischlag/41d15424e7989b936c1609b53edd1390\n",
    "    :return: Quadruple of numpy arrays (60000, 784) training inputs, (60000, 10) one-hot encoded training labels,\n",
    "        (10000, 784) testing inputs and (10000, 10) one-hot encoded testing labels\n",
    "    \"\"\"\n",
    "    IMAGE_SIZE = 28\n",
    "    N_CLASSES = 10\n",
    "    files = {\n",
    "        'X_train': ('train-images-idx3-ubyte.gz', 60000),\n",
    "        'T_train': ('train-labels-idx1-ubyte.gz', 60000),\n",
    "        'X_test': ('t10k-images-idx3-ubyte.gz', 10000),\n",
    "        'T_test': ('t10k-labels-idx1-ubyte.gz', 10000),\n",
    "    }\n",
    "    data = {}\n",
    "    for label, (name, n_images) in files.iteritems():\n",
    "        if not os.path.exists(name):\n",
    "            print('downloading: {}'.format(name))\n",
    "            urllib.urlretrieve('http://yann.lecun.com/exdb/mnist/{}'.format(name), name)\n",
    "        with gzip.open(name) as bytestream:\n",
    "            if label.startswith('X'):\n",
    "\n",
    "                bytestream.read(16)  # header\n",
    "                data[label] = (np.frombuffer(bytestream.read(IMAGE_SIZE * IMAGE_SIZE * n_images),\n",
    "                                             dtype=np.uint8).astype(np.float32) / 255.0).reshape(n_images, -1)\n",
    "            else:\n",
    "                bytestream.read(8)  # header\n",
    "                classes = np.frombuffer(bytestream.read(n_images), dtype=np.uint8).astype(np.int64)\n",
    "                onehot = np.zeros((len(classes), N_CLASSES), dtype=np.float32)\n",
    "                onehot[np.arange(len(classes)), classes] = 1\n",
    "                data[label] = onehot\n",
    "\n",
    "    # standardization\n",
    "    X_train, T_train, X_test, T_test = [data[label] for label in ['X_train', 'T_train', 'X_test', 'T_test']]\n",
    "    m, s = X_train.mean(axis=0), X_train.std(axis=0)\n",
    "    mask = s > 0.0\n",
    "    X_train[:, mask] = (X_train[:, mask] - m[mask]) / s[mask]\n",
    "    X_test[:, mask] = (X_test[:, mask] - m[mask]) / s[mask]\n",
    "\n",
    "    return X_train, T_train, X_test, T_test\n",
    "\n",
    "\n",
    "def plot_MNIST(array, n_cols=10):\n",
    "    \"\"\"\n",
    "    Plots table of MNIST characters with defined number of columns. The number of characters divided by the number of\n",
    "    columns, i.e. the number of rows, must be integer. The call of this function should be followed by plt.show()\n",
    "    in non-interactive matplotlib session.\n",
    "    session.\n",
    "    :param array: input array of shape (number of characters, 784)\n",
    "    :param n_cols: number of table columns\n",
    "    \"\"\"\n",
    "    n, height, width = array.shape[0], 28, 28\n",
    "    n_rows = n // n_cols\n",
    "    assert n == n_rows * n_cols, [n, n_rows * n_cols]\n",
    "    result = (array.reshape(n_rows, n_cols, height, width)\n",
    "              .swapaxes(1, 2)\n",
    "              .reshape(height * n_rows, width * n_cols))\n",
    "    plt.imshow(result, cmap='gray')\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# -------------- LAYERS -----------------\n",
    "# ---------------------------------------\n",
    "\n",
    "class LinearLayer(object):\n",
    "    def __init__(self, n_inputs, n_units, rng, name):\n",
    "        \"\"\"\n",
    "        Linear (dense, fully-connected) layer.\n",
    "        :param n_inputs:\n",
    "        :param n_units:\n",
    "        :param rng: random number generator used for initialization\n",
    "        :param name:\n",
    "        \"\"\"\n",
    "        super(LinearLayer, self).__init__()\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_units = n_units\n",
    "        self.rng = rng\n",
    "        self.name = name\n",
    "        self.initialize()\n",
    "\n",
    "    def has_params(self):\n",
    "        return True\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward message.\n",
    "        :param X: layer inputs, shape (n_samples, n_inputs)\n",
    "        :return: layer output, shape (n_samples, n_units)\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        Y = X.dot(self.W) + self.b\n",
    "        Y.shape[0] == n_samples\n",
    "        Y.shape[1] == self.n_units\n",
    "        return Y\n",
    "\n",
    "    def delta(self, Y, delta_next):\n",
    "        \"\"\"\n",
    "        Computes delta (dl/d(layer inputs)), based on delta from the following layer. The computations involve backward\n",
    "        message.\n",
    "        :param Y: output of this layer (i.e., input of the next), shape (n_samples, n_units)\n",
    "        :param delta_next: delta vector backpropagated from the following ayer, shape (n_samples, n_units)\n",
    "        :return: delta vector from this layer, shape (n_samples, n_inputs)\n",
    "        \"\"\"\n",
    "        n_samples = delta_next.shape[0]\n",
    "        delta = delta_next.dot(self.W.T)\n",
    "        delta.shape[0] == n_samples\n",
    "        delta.shape[1] == self.n_inputs\n",
    "        return delta\n",
    "\n",
    "    def grad(self, X, delta_next):\n",
    "        \"\"\"\n",
    "        Gradient averaged over all samples. The computations involve parameter message.\n",
    "        :param X: layer input, shape (n_samples, n_inputs)\n",
    "        :param delta_next: delta vector backpropagated from the following layer, shape (n_samples, n_units)\n",
    "        :return: a list of two arrays [dW, db] corresponding to gradients of loss w.r.t. weights and biases, the shapes\n",
    "        of dW and db are the same as the shapes of the actual parameters (self.W, self.b)\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        dW = X.T.dot(delta_next) / n_samples\n",
    "        db = delta_next.sum(0) / n_samples\n",
    "        dW.shape == self.W.shape\n",
    "        db.shape == self.b.shape\n",
    "        return [dW,db]\n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\"\n",
    "        Perform He's initialization (https://arxiv.org/pdf/1502.01852.pdf). This method is tuned for ReLU activation\n",
    "        function. Biases are initialized to 1 increasing probability that ReLU is not initially turned off.\n",
    "        \"\"\"\n",
    "        scale = np.sqrt(2.0 / self.n_inputs)\n",
    "        self.W = self.rng.normal(loc=0.0, scale=scale, size=(self.n_inputs, self.n_units))\n",
    "        self.b = np.ones(self.n_units)\n",
    "\n",
    "    def update_params(self, dtheta):\n",
    "        \"\"\"\n",
    "        Updates weighs and biases.\n",
    "        :param dtheta: contains a two element list of weight and bias updates the shapes of which corresponds to self.W\n",
    "        and self.b\n",
    "        \"\"\"\n",
    "        len(dtheta) == 2, len(dtheta)\n",
    "        dW, db = dtheta\n",
    "        dW.shape == self.W.shape, dW.shape\n",
    "        db.shape == self.b.shape, db.shape\n",
    "        self.W += dW\n",
    "        self.b += db\n",
    "\n",
    "\n",
    "class ReLULayer(object):\n",
    "    def __init__(self, name):\n",
    "        super(ReLULayer, self).__init__()\n",
    "        self.name = name\n",
    "\n",
    "    def has_params(self):\n",
    "        return False\n",
    "\n",
    "    def forward(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        n_units = X.shape[1]\n",
    "        Y = np.maximum(X, 0);\n",
    "        Y.shape[0] == n_samples\n",
    "        Y.shape[1] == n_units\n",
    "        return Y\n",
    "\n",
    "    def delta(self, Y, delta_next):\n",
    "        n_samples = delta_next.shape[0]\n",
    "        n_units = delta_next.shape[1]\n",
    "\n",
    "        delta = np.zeros(delta_next.shape)\n",
    "        delta[Y > 0] = delta_next[Y > 0];\n",
    "\n",
    "        delta.shape[0] == n_samples\n",
    "        delta.shape[1] == n_units\n",
    "        return delta\n",
    "\n",
    "\n",
    "class SoftmaxLayer(object):\n",
    "    def __init__(self, name):\n",
    "        super(SoftmaxLayer, self).__init__()\n",
    "        self.name = name\n",
    "\n",
    "    def has_params(self):\n",
    "        return False\n",
    "\n",
    "    def forward(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        n_units = X.shape[1]\n",
    "\n",
    "        Xe = np.exp(X - np.max(X,1).reshape((n_samples,1)))\n",
    "        sms = Xe.sum(1).reshape((n_samples,1))\n",
    "\n",
    "        Y = Xe / sms\n",
    "\n",
    "        Y.shape[0] == n_samples\n",
    "        Y.shape[1] == n_units\n",
    "        return Y\n",
    "\n",
    "\n",
    "    def delta(self, Y, delta_next):\n",
    "        n_samples = delta_next.shape[0]\n",
    "        n_units = delta_next.shape[1]\n",
    "\n",
    "        delta = np.zeros([n_samples,n_units])\n",
    "\n",
    "        for n in xrange(n_samples):\n",
    "            z2 = Y[n,:]\n",
    "            d = delta_next[n,:]\n",
    "            mat = np.tile(d,(n_units,1)) * (np.diag(z2) - z2.reshape((n_units,1)).dot(z2.reshape((1,n_units))) )\n",
    "            dta = np.sum(mat,1).reshape((1,n_units))\n",
    "\n",
    "            delta[n,:] = dta\n",
    "\n",
    "        delta.shape[0] == n_samples\n",
    "        delta.shape[1] == n_units\n",
    "        return delta\n",
    "\n",
    "\n",
    "\n",
    "class LossCrossEntropy(object):\n",
    "    def __init__(self, name):\n",
    "        super(LossCrossEntropy, self).__init__()\n",
    "        self.name = name\n",
    "\n",
    "    def forward(self, X, T):\n",
    "        \"\"\"\n",
    "        Forward message.\n",
    "        :param X: loss inputs (outputs of the previous layer), shape (n_samples, n_inputs), n_inputs is the same as\n",
    "        the number of classes\n",
    "        :param T: one-hot encoded targets, shape (n_samples, n_inputs)\n",
    "        :return: layer output, shape (n_samples, 1)\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        Y = -np.log(X[T == 1]).reshape((n_samples,1))\n",
    "\n",
    "        Y.shape[0] == n_samples\n",
    "        Y.shape[1] == 1\n",
    "        return Y\n",
    "\n",
    "\n",
    "    def delta(self, X, T):\n",
    "        \"\"\"\n",
    "        Computes delta vector for the output layer.\n",
    "        :param X: loss inputs (outputs of the previous layer), shape (n_samples, n_inputs), n_inputs is the same as\n",
    "        the number of classes\n",
    "        :param T: one-hot encoded targets, shape (n_samples, n_inputs)\n",
    "        :return: delta vector from the loss layer, shape (n_samples, n_inputs)\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        n_inputs = X.shape[1]\n",
    "\n",
    "        delta = -T / X\n",
    "\n",
    "        delta.shape[0] == n_samples\n",
    "        delta.shape[1] == n_inputs\n",
    "        return delta\n",
    "\n",
    "\n",
    "\n",
    "class LossCrossEntropyForSoftmaxLogits(object):\n",
    "    def __init__(self, name):\n",
    "        super(LossCrossEntropyForSoftmaxLogits, self).__init__()\n",
    "        self.name = name\n",
    "\n",
    "    def forward(self, X, T):\n",
    "        n_samples = X.shape[0]\n",
    "        n_inputs = X.shape[1]\n",
    "\n",
    "        LogSumXe = np.log(np.exp(X).sum(1)).reshape((n_samples,1))\n",
    "        sk = X[T == 1].reshape((n_samples,1))\n",
    "        Y = -sk + LogSumXe\n",
    "\n",
    "        Y.shape[0] == n_samples\n",
    "        Y.shape[1] == 1\n",
    "        return Y\n",
    "\n",
    "\n",
    "    def delta(self, X, T):\n",
    "\n",
    "        n_samples = X.shape[0]\n",
    "        n_inputs = X.shape[1]\n",
    "\n",
    "        Xe = np.exp(X - np.max(X,1).reshape((n_samples,1)))\n",
    "        sms = Xe.sum(1).reshape((n_samples,1))\n",
    "        P = Xe / sms\n",
    "\n",
    "        delta = P - T\n",
    "\n",
    "        delta.shape[0] == n_samples\n",
    "        delta.shape[1] == n_inputs\n",
    "        return delta\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# -------------- MLP --------------------\n",
    "# ---------------------------------------\n",
    "\n",
    "class MLP(object):\n",
    "    def __init__(self, n_inputs, layers, loss, output_layers=[]):\n",
    "        \"\"\"\n",
    "        MLP constructor.\n",
    "        :param n_inputs:\n",
    "        :param layers: list of layers\n",
    "        :param loss: loss function layer\n",
    "        :param output_layers: list of layers appended to \"layers\" in evaluation phase, parameters of these are not used\n",
    "        in training phase\n",
    "        \"\"\"\n",
    "        self.n_inputs = n_inputs\n",
    "        self.layers = layers\n",
    "        self.output_layers = output_layers\n",
    "        self.loss = loss\n",
    "        self.first_param_layer = layers[-1]\n",
    "        for l in layers:\n",
    "            if l.has_params():\n",
    "                self.first_param_layer = l\n",
    "                break\n",
    "\n",
    "    def propagate(self, X, output_layers=True, last_layer=None):\n",
    "        \"\"\"\n",
    "        Feedforward network propagation\n",
    "        :param X: input data, shape (n_samples, n_inputs)\n",
    "        :param output_layers: controls whether the self.output_layers are appended to the self.layers in evaluatin\n",
    "        :param last_layer: if not None, the propagation will stop at layer with this name\n",
    "        :return: propagated inputs, shape (n_samples, n_units_of_the_last_layer)\n",
    "        \"\"\"\n",
    "        layers = self.layers + (self.output_layers if output_layers else [])\n",
    "        if last_layer is not None:\n",
    "            assert isinstance(last_layer, basestring)\n",
    "            layer_names = map(lambda layer: layer.name, layers)\n",
    "            layers = layers[0: layer_names.index(last_layer) + 1]\n",
    "        for layer in layers:\n",
    "            X = layer.forward(X)\n",
    "\n",
    "        return X\n",
    "\n",
    "    def evaluate(self, X, T):\n",
    "        \"\"\"\n",
    "        Computes loss.\n",
    "        :param X: input data, shape (n_samples, n_inputs)\n",
    "        :param T: target labels, shape (n_samples, n_outputs)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return self.loss.forward(self.propagate(X, output_layers=False), T)\n",
    "\n",
    "    def gradient(self, X, T):\n",
    "        \"\"\"\n",
    "        Computes gradient of loss w.r.t. all network parameters.\n",
    "        :param X: input data, shape (n_samples, n_inputs)\n",
    "        :param T: target labels, shape (n_samples, n_outputs)\n",
    "        :return: a dict of records in which key is the layer.name and value the output of grad function\n",
    "        \"\"\"\n",
    "\n",
    "        Ya = {} # save output of each layer\n",
    "        Xa = {} # save input of each layer\n",
    "\n",
    "        for layer in self.layers: # get inputs & outputs of each layer\n",
    "            Xa[layer.name] = X\n",
    "            X = layer.forward(X)\n",
    "            Ya[layer.name] = X\n",
    "\n",
    "        delta = self.loss.delta(X,T)\n",
    "\n",
    "        G = {}\n",
    "        for layer in reversed(self.layers): # get gradients\n",
    "            if layer.has_params():\n",
    "                G[layer.name] = layer.grad(Xa[layer.name], delta)\n",
    "            delta = layer.delta(Ya[layer.name],delta) # propragate deltas\n",
    "\n",
    "        return G\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# -------------- TRAINING ---------------\n",
    "# ---------------------------------------\n",
    "\n",
    "def accuracy(Y, T):\n",
    "    p = np.argmax(Y, axis=1)\n",
    "    t = np.argmax(T, axis=1)\n",
    "    return np.mean(p == t)\n",
    "\n",
    "\n",
    "def train(net, X_train, T_train, batch_size=1, n_epochs=2, eta=0.1, X_test=None, T_test=None, verbose=False):\n",
    "    \"\"\"\n",
    "    Trains a network using vanilla gradient descent.\n",
    "    :param net:\n",
    "    :param X_train:\n",
    "    :param T_train:\n",
    "    :param batch_size:\n",
    "    :param n_epochs:\n",
    "    :param eta: learning rate\n",
    "    :param X_test:\n",
    "    :param T_test:\n",
    "    :param verbose: prints evaluation for each epoch if True\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    n_samples = X_train.shape[0]\n",
    "    assert T_train.shape[0] == n_samples\n",
    "    assert batch_size <= n_samples\n",
    "    run_info = defaultdict(list)\n",
    "\n",
    "    def process_info(epoch):\n",
    "        loss_test, acc_test = np.nan, np.nan\n",
    "        Y = net.propagate(X_train)\n",
    "        loss_train = net.loss.forward(Y, T_train)\n",
    "        acc_train = accuracy(Y, T_train)\n",
    "        run_info['loss_train'].append(loss_train)\n",
    "        run_info['acc_train'].append(acc_train)\n",
    "        if X_test is not None:\n",
    "            Y = net.propagate(X_test)\n",
    "            loss_test = net.loss.forward(Y, T_test)\n",
    "            acc_test = accuracy(Y, T_test)\n",
    "            run_info['loss_test'].append(loss_test)\n",
    "            run_info['acc_test'].append(acc_test)\n",
    "        if verbose:\n",
    "            print('epoch: {}, loss: {}/{} accuracy: {}/{}'.format(epoch, np.mean(loss_train), np.nanmean(loss_test),\n",
    "                                                                  np.nanmean(acc_train), np.nanmean(acc_test)))\n",
    "        for layer in net.layers:\n",
    "            if(layer.has_params()): # only for linear layers\n",
    "                mn = np.mean(np.abs(layer.W))\n",
    "                run_info[layer.name].append(mn)\n",
    "\n",
    "    process_info('initial')\n",
    "    for epoch in xrange(1, n_epochs + 1):\n",
    "        offset = 0\n",
    "        while offset < n_samples:\n",
    "            last = min(offset + batch_size, n_samples)\n",
    "            if verbose:\n",
    "                print('.', end='')\n",
    "            grads = net.gradient(np.asarray(X_train[offset:last]), np.asarray(T_train[offset:last]))\n",
    "            for layer in net.layers:\n",
    "                if layer.has_params():\n",
    "                    gs = grads[layer.name]\n",
    "                    dtheta = map(lambda g: -eta * g, gs)\n",
    "                    layer.update_params(dtheta)\n",
    "\n",
    "            offset += batch_size\n",
    "        if verbose:\n",
    "            print()\n",
    "        process_info(epoch)\n",
    "    return run_info\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# -------------- EXPERIMENTS ------------\n",
    "# ---------------------------------------\n",
    "\n",
    "def plot_convergence(run_info):\n",
    "    plt.plot(run_info['acc_train'], label='train')\n",
    "    plt.plot(run_info['acc_test'], label='test')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "def plot_test_accuracy_comparison(run_info_dict):\n",
    "    keys = sorted(run_info_dict.keys())\n",
    "    for key in keys:\n",
    "        plt.plot(run_info_dict[key]['acc_test'], label=key)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "def plot_mean_weights(run_info,layers):\n",
    "    for key in layers:\n",
    "        plt.plot(run_info[key]/run_info[key][0], label=key)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('mean weight amplitude')\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "def experiment_XOR():\n",
    "    X, T = load_XOR()\n",
    "    rng = np.random.RandomState(1234)\n",
    "    net = MLP(n_inputs=2,\n",
    "              layers=[\n",
    "                  LinearLayer(n_inputs=2, n_units=4, rng=rng, name='Linear_1'),\n",
    "                  ReLULayer(name='ReLU_1'),\n",
    "                  LinearLayer(n_inputs=4, n_units=2, rng=rng, name='Linear_OUT'),\n",
    "                  SoftmaxLayer(name='Softmax_OUT')\n",
    "              ],\n",
    "              loss=LossCrossEntropy(name='CE'),\n",
    "              )\n",
    "\n",
    "    run_info = train(net, X, T, batch_size=4, eta=0.1, n_epochs=100, verbose=True)\n",
    "    plot_convergence(run_info)\n",
    "    plt.show()\n",
    "    print(net.propagate(X))\n",
    "    plot_2D_classification(X, T, net)\n",
    "    plt.show()\n",
    "    plot_mean_weights(run_info, ['Linear_1','Linear_OUT'])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def experiment_spirals():\n",
    "    X_train, T_train, X_test, T_test = load_spirals()\n",
    "    experiments = (\n",
    "        ('eta = 0.2', 0.2),\n",
    "        ('eta = 1', 1.0),\n",
    "        ('eta = 5', 5.0),\n",
    "    )\n",
    "    run_info_dict = {}\n",
    "    for name, eta in experiments:\n",
    "        rng = np.random.RandomState(1234)\n",
    "        net = MLP(n_inputs=2,\n",
    "                  layers=[\n",
    "                      LinearLayer(n_inputs=2, n_units=10, rng=rng, name='Linear_1'),\n",
    "                      ReLULayer(name='ReLU_1'),\n",
    "                      LinearLayer(n_inputs=10, n_units=3, rng=rng, name='Linear_OUT'),\n",
    "                      SoftmaxLayer(name='Softmax_OUT')\n",
    "                  ],\n",
    "                  loss=LossCrossEntropy(name='CE'),\n",
    "                  )\n",
    "\n",
    "        run_info = train(net, X_train, T_train, batch_size=len(X_train), eta=eta, X_test=X_test, T_test=T_test,\n",
    "                         n_epochs=1000, verbose=True)\n",
    "        run_info_dict[name] = run_info\n",
    "        # plot_spirals(X_train, T_train, net)\n",
    "        # plt.show()\n",
    "        # plot_convergence(run_info)\n",
    "        # plt.show()\n",
    "    plot_test_accuracy_comparison(run_info_dict)\n",
    "    plt.show()\n",
    "    # plt.savefig('spiral.pdf') # you can instead save figure to file\n",
    "\n",
    "\n",
    "def experiment_MNIST():\n",
    "    X_train, T_train, X_test, T_test = load_MNIST()\n",
    "    np.seterr(all='raise', under='warn', over='warn')\n",
    "    rng = np.random.RandomState(1234)\n",
    "    net = MLP(n_inputs=28 * 28,\n",
    "              layers=[\n",
    "                  LinearLayer(n_inputs=28 * 28, n_units=64, rng=rng, name='Linear_1'),\n",
    "                  ReLULayer(name='ReLU_1'),\n",
    "                  LinearLayer(n_inputs=64, n_units=64, rng=rng, name='Linear_2'),\n",
    "                  ReLULayer(name='ReLU_2'),\n",
    "                  LinearLayer(n_inputs=64, n_units=64, rng=rng, name='Linear_3'),\n",
    "                  ReLULayer(name='ReLU_3'),\n",
    "                  LinearLayer(n_inputs=64, n_units=64, rng=rng, name='Linear_4'),\n",
    "                  ReLULayer(name='ReLU_4'),\n",
    "                  LinearLayer(n_inputs=64, n_units=64, rng=rng, name='Linear_5'),\n",
    "                  ReLULayer(name='ReLU_5'),\n",
    "                  LinearLayer(n_inputs=64, n_units=10, rng=rng, name='Linear_OUT'),\n",
    "              ],\n",
    "              loss=LossCrossEntropyForSoftmaxLogits(name='CE'),\n",
    "              output_layers=[SoftmaxLayer(name='Softmax_OUT')]\n",
    "              )\n",
    "\n",
    "    run_info = train(net, X_train, T_train, batch_size=3000, eta=1e-1, X_test=X_test, T_test=T_test, n_epochs=20,\n",
    "                     verbose=True)\n",
    "    plot_convergence(run_info)\n",
    "    plt.show()\n",
    "    plot_mean_weights(run_info,['Linear_1','Linear_2','Linear_3','Linear_4','Linear_5','Linear_OUT'])\n",
    "    plt.show()\n",
    "\n",
    "    #with open('MNIST_run_info.p', 'w') as f:\n",
    "    #    pickle.dump(run_info, f)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #experiment_XOR()\n",
    "\n",
    "    #experiment_spirals()\n",
    "\n",
    "    experiment_MNIST()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:SSU_MLP]",
   "language": "python",
   "name": "conda-env-SSU_MLP-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
